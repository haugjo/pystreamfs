{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from streamfs import streamfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('./datasets/cleaned_german_credit_score.csv')\n",
    "credit_feature_names = np.array(credit_data.drop('Risk', 1).columns)\n",
    "credit_data = np.array(credit_data)\n",
    "\n",
    "har_data = pd.read_csv('./datasets/human_activity_recognition.csv')\n",
    "har_feature_names = np.array(har_data.drop('Activity', 1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAYING' 'SITTING' 'STANDING' 'WALKING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS']\n",
      "[1944 1777 1906 1722 1406 1544]\n",
      "['STANDING' 'WALKING']\n",
      "[1906 1722]\n"
     ]
    }
   ],
   "source": [
    "# Make HAR a dataset for binary classification\n",
    "uniques, counts = np.unique(har_data['Activity'], return_counts=True)\n",
    "print(uniques)\n",
    "print(counts)\n",
    "\n",
    "har_data = har_data[np.isin(har_data['Activity'], ['STANDING', 'WALKING'])]\n",
    "uniques, counts = np.unique(har_data['Activity'], return_counts=True)\n",
    "print(uniques)\n",
    "print(counts)\n",
    "har_data = np.array(har_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Credit Data: (965, 24)\n",
      "Shape HAR: (3628, 563)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Credit Data: {}\".format(credit_data.shape))\n",
    "print(\"Shape HAR: {}\".format(har_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export target\n",
    "credit_X, credit_Y = streamfs.prepare_data(credit_data, 5, False)\n",
    "\n",
    "har_X, har_Y = streamfs.prepare_data(har_data, 562, False)\n",
    "har_X = np.array(har_X, dtype='float')\n",
    "har_Y, _ = pd.factorize(har_Y)\n",
    "har_Y = np.array(har_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "har = dict()\n",
    "har['X'] = har_X\n",
    "har['Y'] = har_Y\n",
    "credit = dict()\n",
    "credit['X'] = credit_X\n",
    "credit['Y'] = credit_Y\n",
    "\n",
    "datasets['har'] = har\n",
    "datasets['credit']= credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FS algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OFS\n",
    "Only binary classification -> Credit Score Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "har\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 200}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 200}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 200}\n",
      "credit\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 5, 'batch_size': 200}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 10, 'batch_size': 200}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 50}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 100}\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'num_features': 20, 'batch_size': 200}\n"
     ]
    }
   ],
   "source": [
    "param = dict()\n",
    "param['algorithm'] = 'knn'  # apply KNN classifier to calculate accuracy per time t\n",
    "param['neighbors'] = 5  # set n_neighbors for KNN\n",
    "\n",
    "ofs_results = dict()\n",
    "\n",
    "\n",
    "# for different data sets\n",
    "for name, data in datasets.items():\n",
    "    X = data['X'].copy()\n",
    "    Y = data['Y'].copy()\n",
    "\n",
    "    Y[Y == 0] = -1  # change 0 to -1, required by ofs\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # different number of features\n",
    "    for n in [5, 10, 20]:\n",
    "        param['num_features'] = n    \n",
    "\n",
    "        # different batch sizes\n",
    "        for b in [50, 100, 200]:\n",
    "            param['batch_size'] = b\n",
    "\n",
    "            print(param)\n",
    "\n",
    "            _, stats = streamfs.simulate_stream(X, Y, 'ofs', param)\n",
    "\n",
    "            ofs_results[\"{};{}F;{}B\".format(name,n,b)] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FSDS\n",
    "Designed for unsupervised learning, can also handle multilabel problems -> Credit score and HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "har\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 200}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 200}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 200}\n",
      "credit\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 5, 'batch_size': 200}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 10, 'batch_size': 200}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 50}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 100}\n",
      "{'b': [], 'ell': 0, 'algorithm': 'knn', 'neighbors': 5, 'k': 2, 'num_features': 20, 'batch_size': 200}\n"
     ]
    }
   ],
   "source": [
    "param = dict()\n",
    "param['b'] = []  # initial sketch matrix\n",
    "param['ell'] = 0  # initial sketch size\n",
    "param['algorithm'] = 'knn'  # apply KNN classifier to calculate accuracy per time t\n",
    "param['neighbors'] = 5  # set n_neighbors for KNN\n",
    "\n",
    "fsds_results = dict()\n",
    "\n",
    "# for different data sets\n",
    "for name, data in datasets.items():\n",
    "    X = data['X'].copy()\n",
    "    Y = data['Y'].copy()\n",
    "\n",
    "    param['k'] = len(np.unique(Y))  # no. of singular values (can be equal to no. of classes)\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # different number of features\n",
    "    for n in [5, 10, 20]:\n",
    "        param['num_features'] = n\n",
    "\n",
    "        # different batch sizes -> batch size for one iteration, must be at least the same size than k!!\n",
    "        for b in [50, 100, 200]:        \n",
    "            param['batch_size'] = b\n",
    "\n",
    "            # reset parameters\n",
    "            param['b'] = []  # initial sketch matrix\n",
    "            param['ell'] = 0  # initial sketch size\n",
    "\n",
    "            print(param)\n",
    "\n",
    "            w, stats = streamfs.simulate_stream(X, Y, 'fsds', param)\n",
    "\n",
    "            fsds_results[\"{};{}F;{}B\".format(name,n,b)] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCNN\n",
    "Is extremely slow for HAR dataset -> thus only Credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "har\n",
      "{'algorithm': 'knn', 'neighbors': 5, 'max_n': 100, 'e_threshold': 3, 'boundary_var_multiplier': 2, 'p_diff_threshold': 50, 'num_features': 5, 'batch_size': 50}\n"
     ]
    }
   ],
   "source": [
    "param = dict()\n",
    "param['algorithm'] = 'knn'  # apply KNN classifier to calculate accuracy per time t\n",
    "param['neighbors'] = 5  # set n_neighbors for KNN\n",
    "\n",
    "# Original parameters from paper\n",
    "param['max_n'] = 100  # maximum number of saved instances per cluster\n",
    "param['e_threshold'] = 3  # error threshold for splitting of a cluster\n",
    "\n",
    "# Additional parameters\n",
    "param['boundary_var_multiplier'] = 2  # multiplier for the var. boundary of the closest centroid (run_mcnn())\n",
    "param['p_diff_threshold'] = 50  # threshold of perc. diff. for split/death rate when drift is assumed (_detect_drift())\n",
    "\n",
    "mcnn_results = dict()\n",
    "\n",
    "# remove HAR, because MCNN is way to slow for this dataset\n",
    "mcnn_datasets = datasets\n",
    "\n",
    "\n",
    "\n",
    "# for different data sets\n",
    "for name, data in datasets.items():\n",
    "    X = data['X'].copy()\n",
    "    Y = data['Y'].copy()\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # different number of features\n",
    "    for n in [5, 10, 20]:\n",
    "        param['num_features'] = n\n",
    "\n",
    "        # different batch sizes -> batch size for one iteration, must be at least the same size than k!!\n",
    "        for b in [50, 100, 200]:        \n",
    "            param['batch_size'] = b\n",
    "\n",
    "            print(param)\n",
    "\n",
    "            w, stats = streamfs.simulate_stream(X, Y, 'mcnn', param)\n",
    "\n",
    "            mcnn_results[\"{};{}F;{}B\".format(name,n,b)] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CancelOut\n",
    "\n",
    "binary classification -> credit score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict()\n",
    "param['algorithm'] = 'knn'  # apply KNN classifier to calculate accuracy per time t\n",
    "param['neighbors'] = 5  # set n_neighbors for KNN\n",
    "\n",
    "canc_results = dict()\n",
    "\n",
    "'''\n",
    "# for different data sets\n",
    "for name, data in datasets.items():\n",
    "    X = data['X'].copy()\n",
    "    Y = data['Y'].copy()\n",
    "'''\n",
    "# for one dataset\n",
    "name = 'credit'\n",
    "X = datasets[name]['X'].copy()\n",
    "Y = datasets[name]['Y'].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(name)\n",
    "\n",
    "# different number of features\n",
    "for n in [5, 10, 20]:\n",
    "    param['num_features'] = n\n",
    "\n",
    "    # different batch sizes -> batch size for one iteration, must be at least the same size than k!!\n",
    "    for b in [50, 100, 200]:        \n",
    "        param['batch_size'] = b\n",
    "\n",
    "        print(param)\n",
    "\n",
    "        w, stats = streamfs.simulate_stream(X, Y, 'nnfs', param)\n",
    "\n",
    "        canc_results[\"{};{}F;{}B\".format(name,n,b)] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./experiment_results/ofs_results.json', 'w') as fp:\n",
    "    json.dump(ofs_results, fp)\n",
    "\n",
    "with open('./experiment_results/fsds_results.json', 'w') as fp:\n",
    "    json.dump(fsds_results, fp)\n",
    "    \n",
    "with open('./experiment_results/mcnn_results.json', 'w') as fp:\n",
    "    json.dump(mcnn_results, fp)\n",
    "\n",
    "with open('./experiment_results/canc_results.json', 'w') as fp:\n",
    "    json.dump(canc_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load data from json\n",
    "with open('./experiment_results/canc_results.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
